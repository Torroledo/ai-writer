{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building Next Generation Writers Using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Environment setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing main libraries for data handling, NLP and DL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "\n",
    "#from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twenty_Thousand_Leagues_under_the_Sea.txt',\n",
       " 'A_Journey_to_the_Interior_of_the_Earth.txt',\n",
       " 'Around_the_World_in_80_Days.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining default folders for I/O data \n",
    "\n",
    "data_directory = '../data/raw/Julio_Verne/'   # data directory containing raw texts\n",
    "save_directory = '../models/'                 # directory to store trained NN models\n",
    "vocabulary_filename = os.path.join(save_directory, \"words_vocabulary.pkl\")\n",
    "\n",
    "filename_list = os.listdir(data_directory)    # filenames of raw text \n",
    "filename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Inputting raw data into python structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWENTY THOUSAND LEAGUES UNDER THE SEA\n",
      "by\n",
      "JULES VERNE\n",
      "PART ONE\n",
      "CHAPTER I\n",
      "\n",
      "A SHIFTING REEF\n",
      "\n",
      "The year 1866 was signalised by a remarkable incident, a mysterious and\n",
      "puzzling phenomenon, which doubtless no one has yet forgotten.  Not to\n",
      "mention rumours which agitated the maritime population and excited the\n",
      "public mind, even in the interior of continents, seafaring men were\n",
      "particularly excited.  Merchants, common sailors, captains of vessels,\n",
      "skippers, both of Europe and America, naval officers of a\n"
     ]
    }
   ],
   "source": [
    "# Lets see how it looks one file\n",
    "\n",
    "file = open(data_directory+filename_list[0], 'r', encoding=\"utf8\")\n",
    "preview = file.read()[0:500]\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TWENTY THOUSAND LEAGUES UNDER THE SEA\\nby\\nJULES VERNE\\nPART ONE\\nCHAPTER I\\n\\nA SHIFTING REEF\\n\\nThe year 1866 was signalised by a remarkable incident, a mysterious and\\npuzzling phenomenon, which doubtless no one has yet forgotten.  Not to\\nmention rumours which agitated the maritime population and excited the\\npublic mind, even in the interior of continents, seafaring men were\\nparticularly excited.  Merchants, common sailors, captains of vessels,\\nskippers, both of Europe and America, naval officers of a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, python processes raw text as follows\n",
    "\n",
    "preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <--- go to slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliar function: creating a list with all words from text \n",
    "\n",
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:43<00:00, 16.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list with all words from text \n",
    "\n",
    "nlp = en_core_web_sm.load() # Loading english processing unit for NLP\n",
    "wordlist = []               # List of possible words used by author in their documents \n",
    "\n",
    "# Looping over all files, loading raw text and using spacy we get the list of all words \n",
    "for file_name in tqdm(filename_list):\n",
    "    input_file = os.path.join(data_directory, file_name)\n",
    "    with codecs.open(input_file, 'r',encoding='utf8') as f:\n",
    "        raw_text = f.read()\n",
    "        \n",
    "    # Creating a doc type from spacy and then reading each line to get words\n",
    "    spacy_document = nlp(raw_text)\n",
    "    wordlist = wordlist + create_wordlist(spacy_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twenty', 'thousand', 'leagues', 'under', 'the', 'sea', '\\r\\n', 'by', '\\r\\n', 'jules', 'verne', '\\r\\n', 'part', 'one', '\\r\\n', 'chapter', 'i', '\\r\\n\\r\\n', 'a', 'shifting', 'reef', '\\r\\n\\r\\n', 'the', 'year', '1866', 'was', 'signalised', 'by', 'a', 'remarkable', 'incident', ',', 'a', 'mysterious', 'and', '\\r\\n', 'puzzling', 'phenomenon', ',', 'which', 'doubtless', 'no', 'one', 'has', 'yet', 'forgotten', '.', ' ', 'not', 'to', '\\r\\n', 'mention', 'rumours', 'which', 'agitated', 'the', 'maritime', 'population', 'and', 'excited', 'the', '\\r\\n', 'public', 'mind', ',', 'even', 'in', 'the', 'interior', 'of', 'continents', ',', 'seafaring', 'men', 'were', '\\r\\n', 'particularly', 'excited', '.', ' ', 'merchants', ',', 'common', 'sailors', ',', 'captains', 'of', 'vessels', ',', '\\r\\n', 'skippers', ',', 'both', 'of', 'europe', 'and', 'america', ',', 'naval', 'officers']\n"
     ]
    }
   ],
   "source": [
    "# How does it look our list of words?\n",
    "\n",
    "print(wordlist[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Vocabulary creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Ranking words according to their frequency \n",
    "\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  14619\n"
     ]
    }
   ],
   "source": [
    "# Mapping from word to index\n",
    "\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "# Size of the vocabulary\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Save the words and vocabulary\n",
    "\n",
    "with open(os.path.join(vocabulary_filename), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Transforming from plain text data to quantitative formatted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setting data preprocessing parameters\n",
    "\n",
    "seq_length = 30       # sequence length\n",
    "sequences_step = 1    # step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 317663\n"
     ]
    }
   ],
   "source": [
    "# Creating sequences of sentences and next word for each one\n",
    "\n",
    "sequences = []\n",
    "next_words = []\n",
    "\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((317663, 30, 14619), (317663, 14619))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating X and Y arrays \n",
    "\n",
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1\n",
    "    \n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(len(X)), 100000, replace=False)\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5. Building DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the DL model\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "WARNING:tensorflow:From /Users/torroledo/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/torroledo/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "model built!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               30466048  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14619)             7499547   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14619)             0         \n",
      "=================================================================\n",
      "Total params: 37,965,595\n",
      "Trainable params: 37,965,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating our model and setting the training parameters\n",
    "\n",
    "rnn_size = 256          # 256 size of RNN\n",
    "#seq_length = 30         # sequence length\n",
    "learning_rate = 0.001   # learning rate\n",
    "\n",
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/torroledo/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 3207s 36ms/step - loss: 6.4043 - categorical_accuracy: 0.0659 - val_loss: 5.9861 - val_categorical_accuracy: 0.0865\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 3115s 35ms/step - loss: 5.8418 - categorical_accuracy: 0.1020 - val_loss: 5.8728 - val_categorical_accuracy: 0.1250\n",
      "\n",
      "Epoch 00002: saving model to ../models//my_model_gen_sentences.02-5.87.hdf5\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 3104s 34ms/step - loss: 5.6063 - categorical_accuracy: 0.1318 - val_loss: 5.7563 - val_categorical_accuracy: 0.1398\n"
     ]
    }
   ],
   "source": [
    "batch_size = 250 # minibatch size\n",
    "num_epochs = 3 # number of epochs\n",
    "\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_directory + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=2)]\n",
    "#fit the model\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "md.save(save_directory + \"/\" + 'my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_directory, \"words_vocabulary.pkl\")\n",
    "\n",
    "with open(os.path.join(save_directory, 'words_vocabulary.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model(save_directory + \"/\" + 'my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <--- go to slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7. Data post processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliar function to sample next character based on the predicted probability distribution \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Post processing parameters\n",
    "\n",
    "words_number = 200                     # Number of words to generate\n",
    "seed_sentences = 'having put his right foot before his left five hundred and'  # Seed sentence to start the generating.\n",
    "\n",
    "# Sentences storage\n",
    "\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "# If the length does not match the requiered for the NN, we add 'verne' several times\n",
    "\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"verne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming seed into a X and y shape and adding to our final text container\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Genereting text word by word \n",
    "\n",
    "for i in range(words_number):\n",
    "    \n",
    "    # Creating one hot encoding representation of the sentence\n",
    "    \n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "\n",
    "    # Sampling the next word from the predicted distribution\n",
    "    \n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.2)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    # Adding the next word to the text\n",
    "    \n",
    "    generated += \" \" + next_word\n",
    "    \n",
    "    # Shifting to the new sentence by one\n",
    "    \n",
    "    sentence = sentence[1:] + [next_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verne verne verne verne verne verne verne verne verne verne verne verne verne verne verne verne verne verne verne having put his right foot before his left five hundred and the \n",
      " the the \n",
      " \n",
      " the \n",
      " \n",
      " whom the the nautilus , the saloon , the \n",
      " engine to the \n",
      " \n",
      " urgently . \n",
      "\n",
      " and the with a \n",
      " \n",
      " start , \n",
      " was the canadian , i have the fresh \n",
      " to \n",
      " left the \n",
      " angle of \n",
      " marvellous the \n",
      " the \n",
      " cough , the other , the be \n",
      " the \n",
      " for the lips \n",
      " the atmosphere .   i \n",
      " nautilus , the the \n",
      " cotta nautilus , the ruhmkorff , \n",
      " that \n",
      " a sea the sea .   the \n",
      " be \n",
      " mr. the captain , \" \" \n",
      "\n",
      " \" it , said , it , and i it \n",
      " , i the \n",
      " , the the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " \n",
      " not \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What do we get?\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## THE END\n",
    "### <--- go to slides ... again "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
